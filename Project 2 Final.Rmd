---
title: "Project 2 Final Version"
author: "Sean Duan, David Reynolds, Joe Connelly"
date: "11/9/2020"
output: pdf_document
always_allow_html: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(tree)
library(ISLR)
library(randomForest)
library(gbm)
library(MCMCpack)
library(mltools)
library(data.table)
set.seed(1)

#first read in the data
xdat<-read.table("Data/Xtrain.txt")
ydat<-read.table("Data/Ytrain.txt")
ydat<-rename(ydat, class = V1)
ydat$class<-as.factor(ydat$class)
d<-cbind(xdat,ydat)
xtest<-read.table("Data/Xtest.txt")

#check to see if any NA's
apply(d, 2, function(x) any(is.na(x)))
#doesn't look like we have to do any more data cleaning?

#prep data for split into test/train
set.seed(1)
train=sample(1:nrow(d),5000)
d_test=d[-train,]
class_test=d_test[,11]
```

## Introduction

looking at our problem, we have 10 columns of numerical data. No domain knowledge has been provided on what the columns represent. We also have a outcome variable that is categorical, with 9 different categories. Again, no domain knowledge has been provided as to what the values mean. Given that we have very little information about our data, and that our number of predictors is significantly lower than our observations n, we decided that a tree based method, a SVM, a KNN, and a neural net were a good range of models to consider

Regarding the data itself, since there was no missing values, no data imputation was necessary before statistical analysis began. Additionally, the only processing we did was to split our training data into a test and training dataset, additionally, we also ensured that our outcome column was correctly assigned as a factor, instead of a continous numerical vector.

## Tree Based Methods 
```{r tree}
#bagging
d_bag=randomForest(class~.,data=d , subset=train,mtry=10,importance =TRUE)
d_bag
#error rate 5.64%
#checking test MSE
yhat.bag = predict (d_bag , newdata=d[-train,])
plot(yhat.bag , class_test)
#importance of variables
importance(d_bag)
varImpPlot(d_bag)
#since gini is interpretationable, 
#we see gini best is v5, v1, v2, and v3

#acc decrease is what we're looking for b/c score improvement
#we see acc best is v1, v9, v2 and v5
table(yhat.bag,class_test)
1-mean(yhat.bag==class_test)
#6.02% class error!

tm <- predict(d_bag, d, "prob")
tmTree<-tm+.000000001
tmTree
Yhot <- one_hot(as.data.table(as.factor(d$class)))
Yhot


tm2 <- predict(d_bag, xtest, "prob")
tmTree2<-tm+.000000001
tmTree2
```

We looked at several different types of tree based methods. Initially, we considered a plain CART, bagging, boosting, and random forest. For each of these methods, we proceeded to do hold-out testing on a subset of our training data, using classification error rate as our criteria. After tuning all of our tree based methods using this criteria, we then selected the best tree based method, which here, is bagging.

Looking at the graphs of accuracy and node purity that we produced from the bagging method, we found that v5, v1, and v2 seemed like they were roughly the most important variables for both accuracy and node purity.

Looking at our table and graph of the outcomes, it is fairly clear that we had excellent classification ability, with a classification error of approximately 6 percent.

## Support Vector Machines 
```{r svm}
#Provided Code
Xtrain = read.table("Data/Xtrain.txt")
Ytrain = read.table("Data/Ytrain.txt", quote="\"", comment.char="")
training = cbind(Ytrain, Xtrain)
names(training)[1] = "Author"
training$Author = as.factor(training$Author)


Yhot = one_hot(as.data.table(as.factor(Ytrain$V1)))
Xtest = read.table("Downloads/Xtest.txt")



#I commented out the parts that take super super super super long to go through.
set.seed(1)

#tune.out1 = tune(svm, Author ~., data = training, kernel = "linear",
#                 ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))
#tune.out2 = tune(svm, Author ~., data = training, kernel = "radial", 
#                 ranges = list(cost = c(.01, 1, 10, 100),
#                               gamma = c(.01, 1, 10, 100)))
#tune.out3 = tune(svm, Author ~., data = training, kernel = "polynomial",
#                 ranges = list(cost = c(.01, 1, 10, 100),
#                               degree = c(2, 3, 4)))
#tune.out4 = tune(svm, Author ~., data = training, kernel = "sigmoid", 
#                 ranges = list(cost = c(0.1, 1, 10, 100),
#                               gamma = c(0.1, 1, 10, 100)))



#summary(tune.out1)
best.mod1 = svm(Author ~., data = training, kernel = "linear",
                cost = 100, probability = T)
ypred1 = predict(best.mod1, Xtrain, probability = T)
probstrain1 = attr(ypred1, "probabilities")

CEtrain1 = -sum(colSums(Yhot*log(probstrain1)))

#summary(tune.out2)
best.mod2 = svm(Author ~ ., data = training, kernel = "radial",
                cost = .00001, gamma = .00001, probability = T)

ypred2 = predict(best.mod2, Xtrain, probability = T)
probstrain2 = attr(ypred2, "probabilities")
CEtrain2 = -sum(colSums(Yhot*log(probstrain2)))

#summary(tune.out3)
best.mod3 = svm(Author ~ ., data = training, kernel = "polynomial",
                cost = .0001, degree = 3, probability = T)
ypred3 = predict(best.mod3, Xtrain, probability = T)
probstrain3 = attr(ypred3, "probabilities")
CEtrain3 = -sum(colSums(Yhot*log(probstrain3)))


#summary(tune.out4)
best.mod4 = svm(Author ~., data = training, kernel = "sigmoid",
                cost = 1, gamma = 1, probability = T)
ypred4 = predict(best.mod4, Xtrain, probability = T)
probstrain4 = attr(ypred4, "probabilities")
CEtrain4 = -sum(colSums(Yhot*log(probstrain4)))

Ppred3 = predict(best.mod3, Xtest, probability = T)
Ppred3 = attr(Ppred3, "probabilities")
```
I initially thought of using a SVM as a classification model for this project for a couple of reasons. From what we learned in class, SVC and SVMs seem to be able to fit pretty complex data, and with us knowing really nothing about the data at all, I thought that it would be interesting to see if a more complex model would work best. Another reason I wanted to use the SVM modeling techniques was because of the usefulness in tuning the parameters. Becasue of how easy it is to tune the parameters, we were able to try and fit many different SVC and SVMs in R. 

Using classification error rate as the criteria, we were able to come up with the best model in predicting the training set. Unfortunately, none of the models tested had a training error rate that was lower than 17 percent. SO, my next task was to use the cross entropy rates to tune the parameters. I used the models that were tuned based on mis-classification rates as approximations, and then plugged and chugged to find the best model cross entropy value. This best model was a polynomial kernel model with a cost of .0001 and a degree of 3. 

## KNN
```{r KNN}
```


## Neural Net
```{r nnets}
```

