---
title: "Author classification using tree-based methods, SVMs, KNN, and neural networks"
author: "Joe Connelly, Sean Duan & David Reynolds"
output:
  html_document:
    df_print: paged
  pdf_document: default
always_allow_html: yes
---

```{r setup, include = FALSE}
```

## Introduction

The dataset analyzed in this report is a subset of data taken from a dataset that includes information about books, with each row representing a different book. The features, which are unnamed, are several objective numeric measures of a text, and the goal of this analysis is to identify the correct author of each book. There are nine different authors (i.e., response levels) and 10 features. The training set contains 10,000 books, and the test set contains 5,000 books. Cross-entropy was used to evaluate model performance.

Given that we have very little information about the predictors and that the number of predictors is significantly lower than the number of observations, we decided that a tree-based method, an SVM, a KNN classifier, and a neural network was a good range of models to consider. Linear discriminant analysis, quadratic discriminant analysis, and multinomial logistic regression were also considered, but these methods did not perform as well, and therefore they will not be discussed.

Since the data contains no missing values, no imputation was necessary. However, it was ensured that the response was correctly assigned as a factor rather than a continuous numeric variable. All other preprocessing steps are explained in the model descriptions.

## Basic data analysis/description/plots

``` {r echo = FALSE, eval = FALSE}
library(e1071)

# Read in the data
x_train<- read.table("Data/Xtrain.txt")

# Correlation matrix
cor(x_train)

# Skewness
e1071::skewness(x_train$V1)
e1071::skewness(x_train$V2)
e1071::skewness(x_train$V3)
e1071::skewness(x_train$V4)
e1071::skewness(x_train$V5)
e1071::skewness(x_train$V6)
e1071::skewness(x_train$V7)
e1071::skewness(x_train$V8)
e1071::skewness(x_train$V9)
e1071::skewness(x_train$V10)
```

Variables 6 and 10 appeared to be highly correlated with one another, but we decided not to remove them, as there are only 10 predictors. Variables 1, 2, 5, and 7 are highly skewed, which may pose a challenge. The fact that there are no categorical predictors should help. Overall, we did not feel that any modifications needed to be made to the data before modeling.

## Tree-based methods 
```{r echo = FALSE, eval = FALSE}
library(tidyverse)
library(tree)
library(ISLR)
library(randomForest)
library(gbm)
library(MCMCpack)
library(mltools)
library(data.table)
library(e1071)
set.seed(1)

#first read in the data
xdat<-read.table("Data/Xtrain.txt")
ydat<-read.table("Data/Ytrain.txt")
ydat<-rename(ydat, class = V1)
ydat$class<-as.factor(ydat$class)
d<-cbind(xdat,ydat)
xtest<-read.table("Data/Xtest.txt")

#check to see if any NA's
apply(d, 2, function(x) any(is.na(x)))
#doesn't look like we have to do any more data cleaning?

#prep data for split into test/train
set.seed(1)
train=sample(1:nrow(d),5000)
d_test=d[-train,]
class_test=d_test[,11]

#bagging
d_bag=randomForest(class~.,data=d , subset=train,mtry=10,importance =TRUE)
d_bag
#error rate 5.64%
#checking test MSE
yhat.bag = predict (d_bag , newdata=d[-train,])
plot(yhat.bag , class_test)
#importance of variables
importance(d_bag)
varImpPlot(d_bag)
#since gini is interpretationable, 
#we see gini best is v5, v1, v2, and v3

#acc decrease is what we're looking for b/c score improvement
#we see acc best is v1, v9, v2 and v5
table(yhat.bag,class_test)
1-mean(yhat.bag==class_test)
#6.02% class error!

tm <- predict(d_bag, d, "prob")
tmTree<-tm+.000000001
tmTree
Yhot <- one_hot(as.data.table(as.factor(d$class)))
Yhot


tm2 <- predict(d_bag, xtest, "prob")
tmTree2<-tm+.000000001
tmTree2
```

We looked at several different types of tree-based methods. Initially, we considered a plain CART, bagging, boosting, and random forest. For each of these methods, we proceeded to do hold-out testing on a subset of the training data, using classification error rate as our criteria. After tuning all of our tree-based methods using this criteria, we selected the best tree-based method, which here was bagging.

Looking at the graphs of accuracy and node purity that we produced from the bagging method, we found that v5, v1, and v2 seemed like they were roughly the most important variables for both accuracy and node purity.

Looking at our table and graph of the outcomes, it is fairly clear that we had excellent classification ability, with a classification error of approximately 6 percent.

## Support vector machines 
```{r echo = FALSE, eval = FALSE}
#Provided Code
Xtrain = read.table("Data/Xtrain.txt")
Ytrain = read.table("Data/Ytrain.txt", quote="\"", comment.char="")
training = cbind(Ytrain, Xtrain)
names(training)[1] = "Author"
training$Author = as.factor(training$Author)


Yhot = one_hot(as.data.table(as.factor(Ytrain$V1)))
Xtest = read.table("Data/Xtest.txt")



#I commented out the parts that take super super super super long to go through.
set.seed(1)

#tune.out1 = tune(svm, Author ~., data = training, kernel = "linear",
#                 ranges = list(cost = c(0.01, 0.1, 1, 10, 100)))
#tune.out2 = tune(svm, Author ~., data = training, kernel = "radial", 
#                 ranges = list(cost = c(.01, 1, 10, 100),
#                               gamma = c(.01, 1, 10, 100)))
#tune.out3 = tune(svm, Author ~., data = training, kernel = "polynomial",
#                 ranges = list(cost = c(.01, 1, 10, 100),
#                               degree = c(2, 3, 4)))
#tune.out4 = tune(svm, Author ~., data = training, kernel = "sigmoid", 
#                 ranges = list(cost = c(0.1, 1, 10, 100),
#                               gamma = c(0.1, 1, 10, 100)))



#summary(tune.out1)
best.mod1 = svm(Author ~., data = training, kernel = "linear",
                cost = 100, probability = T)
ypred1 = predict(best.mod1, Xtrain, probability = T)
probstrain1 = attr(ypred1, "probabilities")

CEtrain1 = -sum(colSums(Yhot*log(probstrain1)))

#summary(tune.out2)
best.mod2 = svm(Author ~ ., data = training, kernel = "radial",
                cost = .00001, gamma = .00001, probability = T)

ypred2 = predict(best.mod2, Xtrain, probability = T)
probstrain2 = attr(ypred2, "probabilities")
CEtrain2 = -sum(colSums(Yhot*log(probstrain2)))

#summary(tune.out3)
best.mod3 = svm(Author ~ ., data = training, kernel = "polynomial",
                cost = .0001, degree = 3, probability = T)
ypred3 = predict(best.mod3, Xtrain, probability = T)
probstrain3 = attr(ypred3, "probabilities")
CEtrain3 = -sum(colSums(Yhot*log(probstrain3)))


#summary(tune.out4)
best.mod4 = svm(Author ~., data = training, kernel = "sigmoid",
                cost = 1, gamma = 1, probability = T)
ypred4 = predict(best.mod4, Xtrain, probability = T)
probstrain4 = attr(ypred4, "probabilities")
CEtrain4 = -sum(colSums(Yhot*log(probstrain4)))

Ppred2 = predict(best.mod3, Xtest, probability = T)
Ppred2 = attr(Ppred2, "probabilities")
```

We initially thought of using an SVM as a classification model for this project for a couple of reasons. From what we learned in class, SVC and SVMs seem to be able to fit pretty complex data, and with us knowing really nothing about the data at all, we thought that it would be interesting to see if a more complex model would work best. Another reason we wanted to use the SVM modeling techniques was because of the usefulness in tuning the parameters. Because of how easy it is to tune the parameters, we were able to try and fit many different SVCs and SVMs. 

Using classification error rate as the criteria, we were able to come up with the best model in predicting the training set. Unfortunately, none of the models tested had a training error rate that was lower than 17 percent. So, our next task was to use cross entropy to tune the parameters. We used the models that were tuned based on misclassification rates as approximations and then plugged and chugged to find the best model in terms of cross-entropy. The best model was a polynomial kernel model with a cost of .0001 and degree of 3. 

## KNN
```{r echo = FALSE, eval = FALSE}
library(caret)
library(class)
library(data.table)
library(dplyr)
library(mltools)

# Read in the data
x_train <- read.table("Data/Xtrain.txt")
y_train <- read.table("Data/Ytrain.txt")
y_train <- rename(y_train, author = V1)
y_train$author <- as.factor(y_train$author)
train <- cbind(x_train, y_train)
x_test <- read.table("Data/Xtest.txt")
y_hot <- one_hot(as.data.table(train$author))

## KNN

# Train the classifier and estimate the test error using 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

knn_accuracy = rep(NA, 15)
for (i in 1:15) {
  knn = caret::train(author ~., data = train, method = "knn", trControl = train_control, 
                     preProcess = c("center", "scale"), tuneGrid = data.frame(k = i))
  knn_accuracy[i] = as.numeric(knn$results[2])
}

# Calculate the training cross-entropy for range of k
set.seed(2)
knn_ce_train = rep(NA, 15)
for (i in 1:15) {
  knn = caret::train(author ~., data = train, method = "knn", preProcess = c("center", "scale"), 
                     tuneGrid = data.frame(k = i))
  knn_pred = predict(knn, newdata = train, type = "prob")
  knn_pred = as.matrix(knn_pred)
  knn_ce_train[i] = -sum(colSums(y_hot*log(knn_pred + 1e-15)))
}

# Train the classifier and estimate the "test CE" using 5-fold cross-validation (2694.341)
K <- 5
I <- 15
set.seed(3)
folds <- sample(1:K, nrow(train), replace = TRUE)
knn_ce_test <- matrix(data = NA, nrow = K, ncol = I)

for (k in 1:K) {
  cv_train = train[folds != k, ]
  cv_test = train[folds == k, ]
  y_hot = one_hot(as.data.table(cv_test$author))
  for (i in 1:15) {
    knn = caret::train(author ~., data = cv_train, method = "knn", preProcess = c("center", "scale"), 
                       tuneGrid = data.frame(k = i))
    knn_pred = predict(knn, newdata = cv_test, type = "prob")
    knn_pred = as.matrix(knn_pred)
    knn_ce_test[k,i] = -sum(colSums(y_hot*log(knn_pred + 1e-15)))
  }
  knn_ce_cv <<- colMeans(knn_ce_test)
}

# Plot the number of neighbors vs. cross-validated CE
k_ce <- data.frame(ce = knn_ce_cv)
k_ce$k <- rownames(k_ce)
k_ce$k <- as.numeric(k_ce$k)

ggplot(data = k_ce, aes(x = k, y = ce, group = 1)) + geom_line() + xlab("K") + ylab("Cross-validated CE") + ggtitle("Number of neighbors vs. cross-validated CE") + theme_minimal()

# Train the best classifier on the full training set
knn <- caret::train(author ~., data = train, method = "knn", preProcess = c("center", "scale"),
                    tuneGrid = data.frame(k = 15))

# Report the confusion matrix and accuracy (0.6704)
confusionMatrix(knn)

# Calculate the training cross-entropy (6223.914)
y_hot <- one_hot(as.data.table(train$author))
knn_pred <- predict(knn, newdata = train, type = "prob")
knn_pred <- as.matrix(knn_pred)
knn_ce <- -sum(colSums(y_hot*log(knn_pred + 1e-15)))

# Save the test prediction probabilities
Ppred3 <- predict(knn, newdata = x_test, type = "prob")
Ppred3 <- as.matrix(Ppred3)
save(Ppred3, file = "Ppred3.RData")
```

KNN was considered because it is very flexible and the bias/variance tradeoff can be controlled directly by varying K. The decision boundary is likely highly nonlinear since there are multiple classes in the response, so KNN is probably better to consider for this problem than LDA, QDA, or multinomial logistic regression.

Before training the KNN classifier, the predictors in the training set were centered and scaled (using "preProcess" in caret). Five-fold cross-validated cross-entropy was used to determine the optimal value for K. In other words, the training data was split into five folds and the average cross-entropy was computed after the classifier was fit five times. This was performed for values of K ranging from 1 to 15. The five-fold cross-validated CE seemed to decrease with each increase in the number of neighbors, so 15 was chosen as the optimal value for K, with a cross-validated CE of 2,694.341. Larger values of K were not considered, as it was believed this could lead to a large decrease in variance at the cost of high bias and the cross-validated CE exhibited a clear negative trend. In addition, it was computationally difficult to compute the cross-validated CE for a large range of K, such as 1 to 50.

The (not cross-validated) training CE increased as K increased, and the 10-fold cross-validated accuracy decreased as K increased. This is likely due to small values of K leading to overfitting. The classifier with K = 15 had an accuracy of approximately 0.67 and a training cross-entropy of 6,223.914. A plot of the number of neighbors vs. cross-validated CE is below.

![](/Users/davidreynolds/Downloads/Documents/GitHub/Data-III-Project-2/knn_plot.jpg)

## Neural network
```{r echo = FALSE, eval = FALSE}
library(ANN2)
library(caret)
library(data.table)
library(dplyr)
library(mltools)

# Read in the data
x_train <- read.table("Data/Xtrain.txt")
y_train <- read.table("Data/Ytrain.txt")
y_train <- rename(y_train, author = V1)
y_train$author <- as.factor(y_train$author)
x_test <- read.table("Data/Xtest.txt")

# Scale the predictors
rescale <- function(x, x_min, x_max) {
  r = (x - x_min)/(x_max - x_min) 
  r[is.nan(r)] = 0
  return(r)
}

x_min <- apply(x_train, 2, min)
x_max <- apply(x_train, 2, max)
x_train_scaled <- t(apply(x_train, 1, rescale, x_min, x_max))
x_test_scaled <- t(apply(x_test, 1, rescale, x_min, x_max))
train <- cbind(x_train_scaled, y_train)

# Train the neural network and estimate the "test CE" using 5-fold cross-validation (1996.641)
K <- 5
set.seed(1)
folds <- sample(1:K, nrow(train), replace = TRUE)
nn_ce <- rep(NA, K)

for (k in 1:K) {
  cv_train = train[folds != k, ]
  cv_test = train[folds == k, ]
  x_cv_train = cv_train[, 1:10]
  y_cv_train = cv_train[, 11]
  x_cv_test = cv_test[, 1:10]
  y_cv_test = cv_test[, 11]
  y_hot = one_hot(as.data.table(y_cv_test))
  set.seed(2)
  nn = neuralnetwork(X = x_cv_train, y = y_cv_train, hidden.layers = c(9, 8, 7), optim.type = "adam",
                     n.epochs = 10, activ.functions = "tanh", learn.rates = 0.05, 
                     val.prop = 0.05, verbose = FALSE)
  nn_pred = predict(nn, newdata = x_cv_test, probability = TRUE)
  nn_probs = as.data.frame(nn_pred$probabilities)
  nn_probs = nn_probs %>% 
    select("1" = class_1, "4" = class_4, "5" = class_5, "6" = class_6, "7" = class_7, "8" = class_8,
           "9" = class_9, "11" = class_11, "12" = class_12)
  nn_probs <- as.matrix(nn_probs)
  nn_ce[k] <- -sum(colSums(y_hot*log(nn_probs + 1e-15)))
}
mean(nn_ce)

# Train the best neural network on the full training set
set.seed(3)
nn <- neuralnetwork(X = x_train_scaled, y = y_train, hidden.layers = c(9, 8, 7), optim.type = "adam", 
                    n.epochs = 10, activ.functions = "tanh", learn.rates = 0.05, val.prop = 0.05, 
                    verbose = FALSE)

# Plot the loss during training
plot(nn)

# Report the confusion matrix and accuracy (0.2272)
nn_pred <- predict(nn, newdata = x_train_scaled)
nn_table <- table(truth = y_train$author, fitted = nn_pred$predictions)
sum(diag(nn_table))/length(y_train$author)

# Calculate the training cross-entropy (10049.33)
nn_pred <- predict(nn, newdata = x_train_scaled, probability = TRUE)
nn_probs <- as.data.frame(nn_pred$probabilities)

nn_probs <- nn_probs %>% 
  select("1" = class_1, "4" = class_4, "5" = class_5, "6" = class_6, "7" = class_7, "8" = class_8,
         "9" = class_9, "11" = class_11, "12" = class_12)

nn_probs <- as.matrix(nn_probs)
y_hot = one_hot(as.data.table(y_train$author))
nn_ce <- -sum(colSums(y_hot*log(nn_probs + 1e-15)))

# Save the test prediction probabilities
Ppred4 <- predict(nn, newdata = x_test_scaled, probability = TRUE)
Ppred4 <- as.data.frame(Ppred4$probabilities)

Ppred4 <- Ppred4 %>% 
  select("1" = class_1, "4" = class_4, "5" = class_5, "6" = class_6, "7" = class_7, "8" = class_8,
         "9" = class_9, "11" = class_11, "12" = class_12)

Ppred4 <- as.matrix(Ppred4)
save(Ppred4, file = "Ppred4.RData")
```

A neural network was considered because it is a very flexible and widely used machine learning algorithm. It is very good at modeling nonlinear relationships, and it is likely that the decision boundary in this problem is not linear.

Before training the neural network, the predictors in the training and test sets were scaled to be between 0 and 1. The parameters were tuned using five-fold cross-validated cross-entropy. In other words, the training data was split into five folds and the average cross-entropy was computed after the neural network was fit five times. This was performed for a range of parameter values.

Note that it was difficult to automate the selection of parameter values in the loop (i.e., using a grid of different values), as RStudio often crashed when calculating the cross-validated cross-entropy alone. Because of this, different values for the number of epochs, learning rate, etc. were plugged into the loop and the cross-validated CEs were compared. Setting the number of hidden layers to three with nine, eight, and seven hidden variables, "optim.type" to "adam," "n.epochs" to 10, "activ.functions" to "tanh," the learning rate to 0.05, and the proportion of the training data used for tracing the loss on the validation set to 0.05 resulted in the lowest cross-validated CE, which was 1,996.641. This model had an accuracy of approximately 0.227 and a training cross-entropy of approximately 10,049.3. A plot of the loss during training is below.

![](/Users/davidreynolds/Downloads/Documents/GitHub/Data-III-Project-2/nn_plot.jpg)

Ppred1.RData is the prediction matrix for bagging, Ppred2.RData is the matrix for SVM, Pred3.RData is the matrix for KNN, and Ppred4.RData is the matrix for the neural network. All four methods performed fairly well on the training set, but it is of course difficult to evaluate performance without test labels. Cross-validated cross-entropy was used to train the KNN classifier and neural network, which may be the best method available to evaluate model performance for this particular problem. In the future, it would probably be useful to find better ways to tune parameters while maintaining computational speed. It can be difficult to train some of these models, especially neural networks, with low computing power.
